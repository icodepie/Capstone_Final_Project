{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f256add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer,  make_column_selector as selector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix, recall_score,\\\n",
    "    accuracy_score, precision_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90196304",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d163147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39898 entries, 0 to 39897\n",
      "Data columns (total 22 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Unnamed: 0                39898 non-null  int64  \n",
      " 1   Age                       39898 non-null  float64\n",
      " 2   Occupation                39898 non-null  object \n",
      " 3   Annual_Income             39898 non-null  float64\n",
      " 4   Monthly_Inhand_Salary     39898 non-null  float64\n",
      " 5   Num_Bank_Accounts         39898 non-null  int64  \n",
      " 6   Num_Credit_Card           39898 non-null  int64  \n",
      " 7   Interest_Rate             39898 non-null  int64  \n",
      " 8   Num_of_Loan               39898 non-null  float64\n",
      " 9   Delay_from_due_date       39898 non-null  int64  \n",
      " 10  Num_of_Delayed_Payment    39898 non-null  float64\n",
      " 11  Changed_Credit_Limit      39898 non-null  float64\n",
      " 12  Num_Credit_Inquiries      39898 non-null  float64\n",
      " 13  Credit_Mix                39898 non-null  object \n",
      " 14  Outstanding_Debt          39898 non-null  float64\n",
      " 15  Credit_Utilization_Ratio  39898 non-null  float64\n",
      " 16  Credit_History_Age        39898 non-null  float64\n",
      " 17  Payment_of_Min_Amount     39898 non-null  object \n",
      " 18  Total_EMI_per_month       39898 non-null  float64\n",
      " 19  Amount_invested_monthly   39898 non-null  float64\n",
      " 20  Payment_Behaviour         39898 non-null  object \n",
      " 21  Credit_Score              39898 non-null  int64  \n",
      "dtypes: float64(12), int64(6), object(4)\n",
      "memory usage: 6.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a418318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Credit_Score','Unnamed: 0'], axis=1)\n",
    "y = df['Credit_Score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec4d024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39898 entries, 0 to 39897\n",
      "Data columns (total 20 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Age                       39898 non-null  float64\n",
      " 1   Occupation                39898 non-null  object \n",
      " 2   Annual_Income             39898 non-null  float64\n",
      " 3   Monthly_Inhand_Salary     39898 non-null  float64\n",
      " 4   Num_Bank_Accounts         39898 non-null  int64  \n",
      " 5   Num_Credit_Card           39898 non-null  int64  \n",
      " 6   Interest_Rate             39898 non-null  int64  \n",
      " 7   Num_of_Loan               39898 non-null  float64\n",
      " 8   Delay_from_due_date       39898 non-null  int64  \n",
      " 9   Num_of_Delayed_Payment    39898 non-null  float64\n",
      " 10  Changed_Credit_Limit      39898 non-null  float64\n",
      " 11  Num_Credit_Inquiries      39898 non-null  float64\n",
      " 12  Credit_Mix                39898 non-null  object \n",
      " 13  Outstanding_Debt          39898 non-null  float64\n",
      " 14  Credit_Utilization_Ratio  39898 non-null  float64\n",
      " 15  Credit_History_Age        39898 non-null  float64\n",
      " 16  Payment_of_Min_Amount     39898 non-null  object \n",
      " 17  Total_EMI_per_month       39898 non-null  float64\n",
      " 18  Amount_invested_monthly   39898 non-null  float64\n",
      " 19  Payment_Behaviour         39898 non-null  object \n",
      "dtypes: float64(12), int64(4), object(4)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6dce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31918, 20) (7980, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf31112b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    20650\n",
       "1    13281\n",
       "3     5967\n",
       "Name: Credit_Score, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84088940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subpipes that scale numeric data and use one hot encoder on categorical \n",
    "subpipe_num = Pipeline(steps=[\n",
    "    \n",
    "    ('ss', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "subpipe_cat = Pipeline(steps=[\n",
    "    \n",
    "    ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "022db912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a column transformer to apply the subpipes and transform the data\n",
    "CT = ColumnTransformer(transformers=[\n",
    "    ('subpipe_num', subpipe_num, selector(dtype_include=np.number)),\n",
    "     ('subpipe_cat', subpipe_cat, selector(dtype_include=object))], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060fd6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use modelwithcv function to get cross val scores\n",
    "class ModelWithCV():\n",
    "    '''Structure to save the model and more easily see its crossvalidation'''\n",
    "    \n",
    "    def __init__(self, model, model_name, X, y, cv_now=True):\n",
    "        self.model = model\n",
    "        self.name = model_name\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # For CV results\n",
    "        self.cv_results = None\n",
    "        self.cv_mean = None\n",
    "        self.cv_median = None\n",
    "        self.cv_std = None\n",
    "        #\n",
    "        if cv_now:\n",
    "            self.cross_validate()\n",
    "        \n",
    "    def cross_validate(self, X=None, y=None, kfolds=10):\n",
    "        '''\n",
    "        Perform cross-validation and return results.\n",
    "        \n",
    "        Args: \n",
    "          X:\n",
    "            Optional; Training data to perform CV on. Otherwise use X from object\n",
    "          y:\n",
    "            Optional; Training data to perform CV on. Otherwise use y from object\n",
    "          kfolds:\n",
    "            Optional; Number of folds for CV (default is 10)  \n",
    "        '''\n",
    "        \n",
    "        cv_X = X if X else self.X\n",
    "        cv_y = y if y else self.y\n",
    "\n",
    "        self.cv_results = cross_val_score(self.model, cv_X, cv_y, cv=kfolds)\n",
    "        self.cv_mean = np.mean(self.cv_results)\n",
    "        self.cv_median = np.median(self.cv_results)\n",
    "        self.cv_std = np.std(self.cv_results)\n",
    "\n",
    "        \n",
    "    def print_cv_summary(self):\n",
    "        cv_summary = (\n",
    "        f'''CV Results for `{self.name}` model:\n",
    "            {self.cv_mean:.5f} ± {self.cv_std:.5f} accuracy\n",
    "        ''')\n",
    "        print(cv_summary)\n",
    "\n",
    "        \n",
    "    def plot_cv(self, ax):\n",
    "        '''\n",
    "        Plot the cross-validation values using the array of results and given \n",
    "        Axis for plotting.\n",
    "        '''\n",
    "        ax.set_title(f'CV Results for `{self.name}` Model')\n",
    "        # Thinner violinplot with higher bw\n",
    "        sns.violinplot(y=self.cv_results, ax=ax, bw=.4)\n",
    "        sns.swarmplot(\n",
    "                y=self.cv_results,\n",
    "                color='orange',\n",
    "                size=10,\n",
    "                alpha= 0.8,\n",
    "                ax=ax\n",
    "        )\n",
    "\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b5220",
   "metadata": {},
   "source": [
    "# Baseline Dummy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "285d0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline for dummy model using most_freq strategy\n",
    "dummy_model_pipe = Pipeline(steps=[\n",
    "    ('ct', CT),\n",
    "    ('dum', DummyClassifier(strategy='most_frequent'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec4293ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5171376652672474"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_model_pipe.fit(X_train, y_train)\n",
    "dummy_model_pipe.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a960366",
   "metadata": {},
   "source": [
    "# Simple Untuned Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fe3c501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    LogisticRegression(random_state=42),\n",
    "    XGBClassifier(random_state=42,use_label_encoder=False,learning_rate=0.1,\n",
    "                    n_estimators=1000,\n",
    "                    max_depth=5,\n",
    "                    min_child_weight=1,\n",
    "                    gamma=0,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective='multi:softmax',\n",
    "                    nthread=4,\n",
    "                    num_class=9,\n",
    "                    seed=27)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e88df53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=3)\n",
      "model score: 0.808\n",
      "model score: 0.622\n",
      "DecisionTreeClassifier(random_state=42)\n",
      "model score: 1.000\n",
      "model score: 0.681\n",
      "RandomForestClassifier(random_state=42)\n",
      "model score: 1.000\n",
      "model score: 0.775\n",
      "AdaBoostClassifier(random_state=42)\n",
      "model score: 0.657\n",
      "model score: 0.656\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "model score: 0.725\n",
      "model score: 0.715\n",
      "LogisticRegression(random_state=42)\n",
      "model score: 0.619\n",
      "model score: 0.624\n",
      "[17:13:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
      "              random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)\n",
      "model score: 0.852\n",
      "model score: 0.750\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    steps = [\n",
    "        ('ct', CT),\n",
    "        ('clf', classifier)\n",
    "    ]\n",
    "    pipeline = Pipeline(steps)\n",
    "    pipeline.fit(X_train, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipeline.score(X_train, y_train))\n",
    "    print(\"model score: %.3f\" % pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53dc7b",
   "metadata": {},
   "source": [
    "# GridSearch LogReg Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d43b31f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'l1_ratio', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8fa84cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = Pipeline(steps=[('CT', CT),\n",
    "                              ('log', LogisticRegression(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f3443fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"log__C\": [.1,.01,.001,.0001],\n",
    "              \"log__solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "              \"log__multi_class\":['auto', 'ovr'],\n",
    "              \"log__max_iter\":[1000]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ddfc4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_log = GridSearchCV(log_model,parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2f18cd3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('CT',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('subpipe_num',\n",
       "                                                                         Pipeline(steps=[('ss',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x000001EB4B2CB6D0>),\n",
       "                                                                        ('subpipe_cat',\n",
       "                                                                         Pipeline(steps=[('ohe',\n",
       "                                                                                          OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                                        sparse=False))]),\n",
       "                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x000001EB4B2CBA30>)])),\n",
       "                                       ('log',\n",
       "                                        LogisticRegression(random_state=42))]),\n",
       "             param_grid={'log__C': [0.1, 0.01, 0.001, 0.0001],\n",
       "                         'log__max_iter': [1000],\n",
       "                         'log__multi_class': ['auto', 'ovr'],\n",
       "                         'log__solver': ['newton-cg', 'lbfgs', 'liblinear',\n",
       "                                         'sag', 'saga']})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_log.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a6243d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log__C': 0.1,\n",
       " 'log__max_iter': 1000,\n",
       " 'log__multi_class': 'auto',\n",
       " 'log__solver': 'liblinear'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_log.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d7e42f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_log_model = Pipeline(steps=[('CT', CT),\n",
    "                              ('log', LogisticRegression(random_state=42,C=0.1,multi_class='auto',solver='liblinear',max_iter=1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4311e9a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('CT',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('subpipe_num',\n",
       "                                                  Pipeline(steps=[('ss',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x000001EB4B2CB6D0>),\n",
       "                                                 ('subpipe_cat',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                 sparse=False))]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x000001EB4B2CBA30>)])),\n",
       "                ('log',\n",
       "                 LogisticRegression(C=0.1, max_iter=1000, random_state=42,\n",
       "                                    solver='liblinear'))])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_log_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e60a41da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6207155836831881"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_log_model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d2d674",
   "metadata": {},
   "source": [
    "# GridSearch XGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "751416f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['objective', 'use_label_encoder', 'base_score', 'booster', 'colsample_bylevel', 'colsample_bynode', 'colsample_bytree', 'enable_categorical', 'gamma', 'gpu_id', 'importance_type', 'interaction_constraints', 'learning_rate', 'max_delta_step', 'max_depth', 'min_child_weight', 'missing', 'monotone_constraints', 'n_estimators', 'n_jobs', 'num_parallel_tree', 'predictor', 'random_state', 'reg_alpha', 'reg_lambda', 'scale_pos_weight', 'subsample', 'tree_method', 'validate_parameters', 'verbosity'])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3519ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = Pipeline(steps=[('CT', CT),\n",
    "                              ('xgb', XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=1000,\n",
    "                    max_depth=5,\n",
    "                    min_child_weight=1,\n",
    "                    gamma=0,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective='multi:softmax',\n",
    "                    nthread=4,\n",
    "                    num_class=9,\n",
    "                    seed=27))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ae8534f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'xgb__max_depth':range(3,10,2),\n",
    " 'xgb__min_child_weight':range(1,6,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "53026a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  60 | elapsed: 21.1min remaining:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 24.7min finished\n",
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:57:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('CT',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('subpipe_num',\n",
       "                                                                         Pipeline(steps=[('ss',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x000001EB4B2CB6D0>),\n",
       "                                                                        ('subpipe_cat',\n",
       "                                                                         Pipeline(steps=[('ohe',\n",
       "                                                                                          OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                                        sparse=False))]),\n",
       "                                                                         <sklearn...\n",
       "                                                      monotone_constraints='()',\n",
       "                                                      n_estimators=1000,\n",
       "                                                      n_jobs=4, nthread=4,\n",
       "                                                      num_class=9,\n",
       "                                                      num_parallel_tree=1,\n",
       "                                                      objective='multi:softprob',\n",
       "                                                      predictor='auto',\n",
       "                                                      random_state=27,\n",
       "                                                      reg_alpha=0, reg_lambda=1,\n",
       "                                                      scale_pos_weight=None,\n",
       "                                                      seed=27, subsample=0.8,\n",
       "                                                      tree_method='exact', ...))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'xgb__max_depth': range(3, 10, 2),\n",
       "                         'xgb__min_child_weight': range(1, 6, 2)},\n",
       "             scoring='f1_micro', verbose=10)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_xgb = GridSearchCV(estimator = xgb_model, param_grid = param_test1, scoring='f1_micro',n_jobs=-1,verbose = 10, cv=5)\n",
    "\n",
    "gs_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e522ee59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xgb__max_depth': 7, 'xgb__min_child_weight': 1}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "df06ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_first_model = Pipeline(steps=[('CT', CT),\n",
    "                              ('xgb', XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=1000,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=1,\n",
    "                    gamma=0,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective='multi:softmax',\n",
    "                    nthread=4,\n",
    "                    num_class=9,\n",
    "                    seed=27))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "441963fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:41:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('CT',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('subpipe_num',\n",
       "                                                  Pipeline(steps=[('ss',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x000001EB4B2CB6D0>),\n",
       "                                                 ('subpipe_cat',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                 sparse=False))]),\n",
       "                                                  <sklearn.compose._column_transformer...\n",
       "                               interaction_constraints='', learning_rate=0.1,\n",
       "                               max_delta_step=0, max_depth=7,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=1000,\n",
       "                               n_jobs=4, nthread=4, num_class=9,\n",
       "                               num_parallel_tree=1, objective='multi:softprob',\n",
       "                               predictor='auto', random_state=27, reg_alpha=0,\n",
       "                               reg_lambda=1, scale_pos_weight=None, seed=27,\n",
       "                               subsample=0.8, tree_method='exact', ...))])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_first_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0c7a8666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991227520521336"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_first_model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ca59c726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:45:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:46:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:47:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:48:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:48:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:49:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:50:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:51:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:52:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:53:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CV Results for `xgb_first_model` model:\n",
      "            0.77408 ± 0.00533 accuracy\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "xgb_first_model_val = ModelWithCV(xgb_first_model, 'xgb_first_model', X_train, y_train)\n",
    "xgb_first_model_val.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "11b163ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  15 | elapsed:  3.4min remaining: 22.0min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  15 | elapsed:  3.4min remaining:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed:  3.4min remaining:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:  3.5min remaining:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed:  4.1min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:  5.3min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.4min finished\n",
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:00:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('CT',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('subpipe_num',\n",
       "                                                                         Pipeline(steps=[('ss',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x000001EB4B2CB6D0>),\n",
       "                                                                        ('subpipe_cat',\n",
       "                                                                         Pipeline(steps=[('ohe',\n",
       "                                                                                          OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                                        sparse=False))]),\n",
       "                                                                         <sklearn...\n",
       "                                                      missing=nan,\n",
       "                                                      monotone_constraints='()',\n",
       "                                                      n_estimators=1000,\n",
       "                                                      n_jobs=4, nthread=4,\n",
       "                                                      num_class=9,\n",
       "                                                      num_parallel_tree=1,\n",
       "                                                      objective='multi:softprob',\n",
       "                                                      predictor='auto',\n",
       "                                                      random_state=27,\n",
       "                                                      reg_alpha=0, reg_lambda=1,\n",
       "                                                      scale_pos_weight=None,\n",
       "                                                      seed=27, subsample=0.8,\n",
       "                                                      tree_method='exact', ...))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'xgb__reg_alpha': [1e-05, 0.01, 0.1, 1, 100]},\n",
       "             scoring='f1_micro', verbose=10)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {\n",
    " 'xgb__reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gs_xgb2 = GridSearchCV(estimator = xgb_first_model, param_grid = param_test2, scoring='f1_micro',n_jobs=-1,verbose = 10, cv=3)\n",
    "gs_xgb2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a4962da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xgb__reg_alpha': 0.01}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_xgb2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2c19cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_second_model = Pipeline(steps=[('CT', CT),\n",
    "                              ('xgb', XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=1000,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=1,\n",
    "                    gamma=0,\n",
    "                    reg_alpha=0.01,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective='multi:softmax',\n",
    "                    nthread=4,\n",
    "                    num_class=9,\n",
    "                    seed=27))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ce2e5090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:04:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('CT',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('subpipe_num',\n",
       "                                                  Pipeline(steps=[('ss',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x000001EB4B2CB6D0>),\n",
       "                                                 ('subpipe_cat',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                 sparse=False))]),\n",
       "                                                  <sklearn.compose._column_transformer...\n",
       "                               interaction_constraints='', learning_rate=0.1,\n",
       "                               max_delta_step=0, max_depth=7,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=1000,\n",
       "                               n_jobs=4, nthread=4, num_class=9,\n",
       "                               num_parallel_tree=1, objective='multi:softprob',\n",
       "                               predictor='auto', random_state=27,\n",
       "                               reg_alpha=0.01, reg_lambda=1,\n",
       "                               scale_pos_weight=None, seed=27, subsample=0.8,\n",
       "                               tree_method='exact', ...))])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_second_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "29f70684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999154082335986"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_second_model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d971a350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:06:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:07:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:08:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:10:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:11:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:11:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:12:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:13:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kev\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:14:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CV Results for `xgb_second_model` model:\n",
      "            0.77499 ± 0.00556 accuracy\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "xgb_second_model_val = ModelWithCV(xgb_second_model, 'xgb_second_model', X_train, y_train)\n",
    "xgb_second_model_val.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a4e659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_third_model = Pipeline(steps=[('CT', CT),\n",
    "                              ('xgb', XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=1000,\n",
    "                    max_depth=5,\n",
    "                    min_child_weight=.1,\n",
    "                    max_delta_step=.5,\n",
    "                    gamma=.5,\n",
    "                    reg_alpha=1,\n",
    "                    reg_lambda = .2,\n",
    "                    subsample=.3,\n",
    "                    colsample_bytree=.3,\n",
    "                    objective='multi:softmax',\n",
    "                    num_class=3,\n",
    "                    seed=27))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a86ca913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:32:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('CT',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('subpipe_num',\n",
       "                                                  Pipeline(steps=[('ss',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000021165AC4850>),\n",
       "                                                 ('subpipe_cat',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                 sparse=False))]),\n",
       "                                                  <sklearn.compose._column_transformer...\n",
       "                               interaction_constraints='', learning_rate=0.1,\n",
       "                               max_delta_step=0.5, max_depth=5,\n",
       "                               min_child_weight=0.1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=1000,\n",
       "                               n_jobs=8, num_class=3, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', predictor='auto',\n",
       "                               random_state=27, reg_alpha=1, reg_lambda=0.2,\n",
       "                               scale_pos_weight=None, seed=27, subsample=0.3,\n",
       "                               tree_method='exact', validate_parameters=1, ...))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_third_model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfe6460b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8705745974058525"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_third_model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98f423d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7543859649122807"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_third_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ad81c",
   "metadata": {},
   "source": [
    "# Gridsearch RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "26df0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model = Pipeline(steps=[('CT', CT),\n",
    "                              ('rfc',RandomForestClassifier(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "7239ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specified ranges for different parameters\n",
    "params = {\n",
    "    \"rfc__n_estimators\":[5,10,50,100,250,500],\n",
    "    \"rfc__max_depth\":[2,4,8,16,32]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "fb23c8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('CT',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('subpipe_num',\n",
       "                                                                         Pipeline(steps=[('ss',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x000001EB4B2CB6D0>),\n",
       "                                                                        ('subpipe_cat',\n",
       "                                                                         Pipeline(steps=[('ohe',\n",
       "                                                                                          OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                                        sparse=False))]),\n",
       "                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x000001EB4B2CBA30>)])),\n",
       "                                       ('rfc',\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             param_grid={'rfc__max_depth': [2, 4, 8, 16, 32],\n",
       "                         'rfc__n_estimators': [5, 10, 50, 100, 250, 500]})"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = GridSearchCV(rfc_model,params,cv=5)\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "694ec724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__max_depth': 32, 'rfc__n_estimators': 500}"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee963609",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model = Pipeline(steps=[('CT', CT),\n",
    "                              ('rfc',RandomForestClassifier(random_state=42,n_estimators=1000,max_features='auto',max_depth=12,criterion='gini'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30b62dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('CT',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('subpipe_num',\n",
       "                                                  Pipeline(steps=[('ss',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000021165AC4850>),\n",
       "                                                 ('subpipe_cat',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                 sparse=False))]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000021165AC46A0>)])),\n",
       "                ('rfc',\n",
       "                 RandomForestClassifier(max_depth=12, n_estimators=1000,\n",
       "                                        random_state=42))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d317c0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8012093489567016"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "062056b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7288220551378446"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842bfee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
